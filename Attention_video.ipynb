{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention-video.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0ZiOBOQYFovB",
        "8PGUeEwkFO8n",
        "q3Zka-82FWfk"
      ],
      "authorship_tag": "ABX9TyNvJybShJoI6b0A7CPM9MtP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jasmine216/VMAF-Optimization/blob/master/Attention_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DR8l8u7xe9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9acec2c4-90d7-45ea-d561-9c58133b8f8e"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2T4s7hyvzl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "53b9e8f8-fa4c-4af7-fab1-913648bcdfad"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 9648825334460408353, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 3884799868730972366\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 11379259582280096335\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15701463552\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 3458238356041388606\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0o_GxslF96w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "5775c5be-7f79-4d99-8b4c-c8535832e639"
      },
      "source": [
        "pip list |grep tensor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorboard              2.2.2          \n",
            "tensorboard-plugin-wit   1.6.0.post3    \n",
            "tensorboardcolab         0.0.22         \n",
            "tensorflow               2.2.0          \n",
            "tensorflow-addons        0.8.3          \n",
            "tensorflow-datasets      2.1.0          \n",
            "tensorflow-estimator     2.2.0          \n",
            "tensorflow-gcs-config    2.2.0          \n",
            "tensorflow-hub           0.8.0          \n",
            "tensorflow-metadata      0.22.2         \n",
            "tensorflow-privacy       0.2.2          \n",
            "tensorflow-probability   0.10.0         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZiOBOQYFovB",
        "colab_type": "text"
      },
      "source": [
        "# requirement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_HfD6eyDnek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "8ea81d2a-670b-48ed-8f1b-d4c30c5286d8"
      },
      "source": [
        "!pip uninstall tensorflow==1.13.1\n",
        "!pip install tensorflow-gpu==1.13.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.3.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.3.0\n",
            "Collecting tensorflow-gpu==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 45kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.30.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.18.5)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 41.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (49.2.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.1.0)\n",
            "Installing collected packages: mock, tensorflow-estimator, keras-applications, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.2 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOn_wwZn68gV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import time\n",
        "import h5py\n",
        "#import OMCNN_2CLSTM as Network  # define the CNN\n",
        "import random\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PGUeEwkFO8n",
        "colab_type": "text"
      },
      "source": [
        "# BasicNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQoeYk0UFEU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicNet(object):\n",
        "  weight_decay = 5*1e-6\n",
        "  weight_init = 0.1 #weight init for biasis\n",
        "  leaky_alpha = 0.1\n",
        "  is_training = False\n",
        "  def __init__(self):\n",
        "    self.pretrain_var_collection = []\n",
        "    self.initial_var_collection = []\n",
        "    self.trainable_var_collection = []\n",
        "    self.var_rename = {}\n",
        "    # self.weight_decay = FLAGS.weight_decay\n",
        "    # self.weight_init = FLAGS.weight_init\n",
        "    # self.leaky_alpha = FLAGS.leaky_alpha\n",
        "\n",
        "  def leaky_relu(self, x, alpha, dtype=tf.float32):\n",
        "    \"\"\"leaky relu\n",
        "    if x > 0:\n",
        "      return x\n",
        "    else:\n",
        "      return alpha * x\n",
        "    Args:\n",
        "      x : Tensor\n",
        "      alpha: float\n",
        "    Return:\n",
        "      y : Tensor\n",
        "    \"\"\"\n",
        "    x = tf.cast(x, dtype=dtype)\n",
        "    bool_mask = (x > 0)\n",
        "    mask = tf.cast(bool_mask, dtype=dtype)\n",
        "    return 1.0 * mask * x + alpha * (1 - mask) * x\n",
        "\n",
        "  def get_bilinear(self, f_shape):\n",
        "        width = f_shape[1]\n",
        "        heigh = f_shape[0]\n",
        "        f = width//2 + 1\n",
        "        c = (2 * f - 1 - f % 2) / (2.0 * f)\n",
        "        bilinear = np.zeros([f_shape[0], f_shape[1]])\n",
        "        for x in range(width):\n",
        "            for y in range(heigh):\n",
        "                value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n",
        "                bilinear[x, y] = value\n",
        "        weights = np.zeros(f_shape)\n",
        "        bilinear = bilinear / (np.sum(bilinear)*f_shape[2])\n",
        "        for i in range(f_shape[2]):\n",
        "          for j in range(f_shape[3]):\n",
        "            weights[:, :, i, j] = bilinear\n",
        "\n",
        "\n",
        "        return weights\n",
        "\n",
        "  def get_centermask(self,f_shape): # shape[batchsize, height, width, channals]\n",
        "    width = f_shape[2]\n",
        "    heigh = f_shape[1]\n",
        "    midw = width//2\n",
        "    midh = heigh//2\n",
        "    distmatrix = np.zeros([heigh, width])\n",
        "    for x in range(width):\n",
        "      for y in range(heigh):\n",
        "        value = np.sqrt((x - midw)**2+(y - midh)**2)\n",
        "        distmatrix[x, y] = value\n",
        "    distmatrix = distmatrix / np.max(distmatrix)\n",
        "    distmatrix = 1 -  distmatrix\n",
        "    distmatrix = distmatrix[np.newaxis,...,np.newaxis]\n",
        "    # distmatrix = tf.expand_dims(distmatrix, 0)\n",
        "    # distmatrix = tf.expand_dims(distmatrix, 3)\n",
        "    # for a in range(f_shape[0]):\n",
        "    #   for b in range(f_shape[3]):\n",
        "    #     mask[a, :, :, b] = distmatrix\n",
        "    return distmatrix\n",
        "\n",
        "\n",
        "  def _activation_summary(self,x, name = None):\n",
        "    \"\"\"Helper to create summaries for activations.\n",
        "\n",
        "    Creates a summary that provides a histogram of activations.\n",
        "    Creates a summary that measure the sparsity of activations.\n",
        "\n",
        "    Args:\n",
        "      x: Tensor\n",
        "    Returns:\n",
        "      nothing\n",
        "    \"\"\"\n",
        "    if name is None:\n",
        "      tensor_name = x.op.name\n",
        "    else:\n",
        "      tensor_name = name\n",
        "    tf.summary.histogram(tensor_name + '/activations', x)\n",
        "    tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
        "\n",
        "  def _variable_summaries(var):\n",
        "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
        "    if not tf.get_variable_scope().reuse:\n",
        "      name = var.op.name\n",
        "      with tf.name_scope('summaries'):\n",
        "        mean = tf.reduce_mean(var)\n",
        "        tf.summary.scalar(name + '/mean', mean)\n",
        "        stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
        "        tf.summary.scalar(name + '/sttdev', stddev)\n",
        "        l2norm = tf.sqrt(tf.reduce_sum(tf.square(var)))\n",
        "        tf.summary.scalar(name + '/l2norm', l2norm)\n",
        "        tf.summary.histogram(name, var)\n",
        "\n",
        "\n",
        "\n",
        "  # def _variable_summary(self,var):\n",
        "  #   \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "  #   varriable_name=var.op.name\n",
        "  #   mean = tf.reduce_mean(var)\n",
        "  #   tf.summary.scalar(varriable_name+'/mean', mean)\n",
        "  #   stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "  #   tf.summary.scalar(varriable_name+'/stddev', stddev)\n",
        "  #   l2norm = tf.sqrt(tf.reduce_sum(tf.square(var)))\n",
        "  #   tf.summary.scalar(varriable_name + '/l2norm', l2norm)\n",
        "  #   tf.summary.histogram(varriable_name+'/histogram', var)\n",
        "\n",
        "  def _variable_on_cpu(self,name, shape, initializer, pretrain = False, trainable = True):\n",
        "    \"\"\"Helper to create a Variable stored on CPU memory.\n",
        "\n",
        "    Args:\n",
        "      name: name of the variable\n",
        "      shape: list of ints\n",
        "      initializer: initializer for Variable\n",
        "\n",
        "    Returns:\n",
        "      Variable Tensor\n",
        "    \"\"\"\n",
        "    with tf.device('/cpu:0'):\n",
        "      var = tf.get_variable(name, shape, initializer=initializer)\n",
        "      #self.var_rename['inference/' + var.op.name] = var #for translate\n",
        "      # print(var.op.name)\n",
        "    if  tf.get_variable_scope().reuse == False:\n",
        "        if pretrain:\n",
        "          self.pretrain_var_collection.append(var)\n",
        "        else:\n",
        "          self.initial_var_collection.append(var)\n",
        "        if trainable:\n",
        "          self.trainable_var_collection.append(var)\n",
        "\n",
        "    return var\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def _variable_with_weight_decay(self,name, shape, wd, pretrain = False, bilinear = False, trainable = True):\n",
        "    \"\"\"Helper to create an initialized Variable with weight decay.\n",
        "\n",
        "    Note that the Variable is initialized with a truncated normal distribution.\n",
        "    A weight decay is added only if one is specified.\n",
        "\n",
        "    Args:\n",
        "      name: name of the variable\n",
        "      shape: list of ints\n",
        "      stddev: standard deviation of a truncated Gaussian\n",
        "      wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
        "          decay is not added for this Variable.\n",
        "\n",
        "    Returns:\n",
        "      Variable Tensor\n",
        "    \"\"\"\n",
        "\n",
        "    if bilinear:\n",
        "      weights = self.get_bilinear(shape)\n",
        "      #print(weights[:,:,1,1])\n",
        "      initializer = tf.constant_initializer(value=weights, dtype=tf.float32)\n",
        "    else:\n",
        "      initializer = tf.contrib.layers.xavier_initializer()\n",
        "    var = self._variable_on_cpu(name, shape, initializer, pretrain, trainable)\n",
        "\n",
        "    if wd and not tf.get_variable_scope().reuse:\n",
        "      weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
        "      #weight_decay = tf.reduce_mean((var**2)*wd, name='weight_loss')\n",
        "      weight_decay.set_shape([])\n",
        "      tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weight_decay)\n",
        "    return var\n",
        "\n",
        "  def conv_layer(self,scope_name,inputs, kernel_size,num_features, stride=1, linear = False, pretrain = False, batchnormalization = False, trainable = True):\n",
        "    \"\"\"convolutional layer\n",
        "    Args:\n",
        "    input: 4 - D\n",
        "    tensor[batch_size, height, width, depth]\n",
        "    scope: variable_scope\n",
        "    name\n",
        "    kernel_size: [k_height, k_width]\n",
        "    stride: int32\n",
        "\n",
        "  Return:\n",
        "  output: 4 - D\n",
        "  tensor[batch_size, height / stride, width / stride, out_channels]\n",
        "  \"\"\"\n",
        "    with tf.variable_scope(scope_name) as scope:\n",
        "      input_channels = inputs.get_shape()[3].value\n",
        "      weights = self._variable_with_weight_decay('weights', shape=[kernel_size,kernel_size,input_channels,num_features], wd=self.weight_decay, pretrain = pretrain, trainable = trainable)\n",
        "      biases = self._variable_on_cpu('biases',[num_features],tf.constant_initializer(self.weight_init), pretrain, trainable)\n",
        "      pad_size = kernel_size // 2\n",
        "      pad_mat = np.array([[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]])\n",
        "      inputs_pad = tf.pad(inputs, pad_mat)\n",
        "      conv = tf.nn.conv2d(inputs_pad, weights, strides=[1, stride, stride, 1], padding='VALID')\n",
        "      self.testvar = biases\n",
        "      conv_biased = tf.nn.bias_add(conv, biases,  name='linearout')\n",
        "      if batchnormalization:\n",
        "        conv_biased = tf.layers.batch_normalization(conv_biased, training = self.is_training)\n",
        "      if linear:\n",
        "        return conv_biased\n",
        "      conv_rect = self.leaky_relu(conv_biased,self.leaky_alpha )\n",
        "      # scope.reuse_variables()\n",
        "      return conv_rect\n",
        "\n",
        "  def transpose_conv_layer(self,scope_name,inputs, kernel_size,num_features, stride, linear = False, pretrain = False, trainable = True):\n",
        "    #Filter size:A 4-D Tensor with the same type as value and shape [height, width, output_channels, in_channels],different from conv.\n",
        "    with tf.variable_scope(scope_name) as scope:\n",
        "      input_channels = inputs.get_shape()[3].value\n",
        "      weights = self._variable_with_weight_decay('weights', shape=[kernel_size,kernel_size,num_features,input_channels], wd=self.weight_decay, pretrain = pretrain, bilinear = False, trainable = trainable)\n",
        "      biases = self._variable_on_cpu('biases',[num_features],tf.constant_initializer(self.weight_init), pretrain, trainable)\n",
        "      # scope.reuse_variables()\n",
        "      batch_size = tf.shape(inputs)[0]\n",
        "      output_shape = tf.stack([tf.shape(inputs)[0], tf.shape(inputs)[1]*stride, tf.shape(inputs)[2]*stride, num_features])\n",
        "      conv = tf.nn.conv2d_transpose(inputs, weights, output_shape, strides=[1,stride,stride,1], padding='SAME')\n",
        "      conv_biased = tf.nn.bias_add(conv, biases, name='linearout')\n",
        "      if linear:\n",
        "        return conv_biased\n",
        "      conv_rect = self.leaky_relu(conv_biased,self.leaky_alpha )\n",
        "      return conv_rect\n",
        "\n",
        "  def max_pool(self,scope_name, input, kernel_size, stride):\n",
        "    \"\"\"max_pool layer\n",
        "    Args:\n",
        "      input: 4-D tensor [batch_zie, height, width, depth]\n",
        "      kernel_size: [k_height, k_width]\n",
        "      stride: int32\n",
        "    Return:\n",
        "      output: 4-D tensor [batch_size, height/stride, width/stride, depth]\n",
        "     \"\"\"\n",
        "    with tf.variable_scope(scope_name) as scope:\n",
        "      pool = tf.nn.max_pool(input, ksize=[1, kernel_size, kernel_size, 1], strides=[1, stride, stride, 1], padding='SAME',name='pooling')\n",
        "    return pool\n",
        "\n",
        "\n",
        "  def fc_layer(self,scope_name,inputs, hiddens, flat = False, linear = False, pretrain = False, trainable = True):\n",
        "    with tf.variable_scope(scope_name) as scope:\n",
        "      input_shape = inputs.get_shape().as_list()\n",
        "      if flat:\n",
        "        dim = input_shape[1]*input_shape[2]*input_shape[3]\n",
        "        inputs_processed = tf.reshape(inputs, [-1,dim])\n",
        "      else:\n",
        "        dim = input_shape[1]\n",
        "        inputs_processed = inputs\n",
        "\n",
        "      weights = self._variable_with_weight_decay('weights', shape=[dim,hiddens], wd=self.weight_decay, pretrain=pretrain, trainable = trainable)\n",
        "      biases = self._variable_on_cpu('biases', [hiddens], tf.constant_initializer(self.weight_init), pretrain, trainable)\n",
        "      # scope.reuse_variables()\n",
        "      ip = tf.add(tf.matmul(inputs_processed, weights), biases, name='linearout')\n",
        "      if linear:\n",
        "       return ip\n",
        "      fc_relu =  self.leaky_relu(ip,self.leaky_alpha )\n",
        "      return fc_relu\n",
        "\n",
        "  def leaky_conv(self, net_in, n_filter, filter_size, strides, name, pretrain=True, trainable=True):\n",
        "    return self.conv_layer(scope_name=name, inputs=net_in, kernel_size=filter_size, num_features=n_filter,\n",
        "                               stride=strides, linear=False, pretrain=pretrain,\n",
        "                               batchnormalization=False, trainable=trainable)\n",
        "\n",
        "  def leaky_deconv(self, name, input_layer, n_filter, out_size):\n",
        "    return self.transpose_conv_layer(scope_name=name, inputs=input_layer, kernel_size=4, num_features=n_filter,\n",
        "                                         stride=2, linear=False, pretrain=True, trainable=True)\n",
        "\n",
        "  def upsample(self, name, input_layer, out_size):\n",
        "    return self.transpose_conv_layer(scope_name=name, inputs=input_layer, kernel_size=4, num_features=2,\n",
        "                                         stride=2, linear=True, pretrain=True, trainable=True)\n",
        "\n",
        "  def flow(self, name, input_layer, filter_size=3):\n",
        "        return self.conv_layer(scope_name=name, inputs=input_layer, kernel_size=filter_size, num_features=2,\n",
        "                               stride=1, linear=True, pretrain=True, batchnormalization=False, trainable=True)\n",
        "\n",
        "  def conv_mask(self, net_in, mask):\n",
        "      tempsize = net_in.get_shape().as_list()\n",
        "      net_in_mask = tf.image.resize_images(mask, [tempsize[1], tempsize[2]])\n",
        "      #print(net_in_mask.get_shape().as_list())\n",
        "      return net_in * net_in_mask\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3Zka-82FWfk",
        "colab_type": "text"
      },
      "source": [
        "# BasicConvLSTMCell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmJPLvqEFKoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class ConvRNNCell(object):\n",
        "    \"\"\"Abstract object representing an Convolutional RNN cell.\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, inputs, state, scope=None):\n",
        "        \"\"\"Run this RNN cell on inputs, starting from the given state.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Abstract method\")\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        \"\"\"size(s) of state(s) used by this cell.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Abstract method\")\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        \"\"\"Integer or TensorShape: size of outputs produced by this cell.\"\"\"\n",
        "        raise NotImplementedError(\"Abstract method\")\n",
        "\n",
        "    def zero_state(self, batch_size, hiddennum, dtype):\n",
        "        \"\"\"Return zero-filled state tensor(s).\n",
        "        Args:\n",
        "          batch_size: int, float, or unit Tensor representing the batch size.\n",
        "          dtype: the data type to use for the state.\n",
        "        Returns:\n",
        "          tensor of shape '[batch_size x shape[0] x shape[1] x num_features]\n",
        "          filled with zeros\n",
        "        \"\"\"\n",
        "\n",
        "        shape = self.shape\n",
        "        num_features = self.num_features\n",
        "        zeros = tf.zeros([batch_size, shape[0], shape[1], num_features * hiddennum])\n",
        "        return zeros\n",
        "\n",
        "\n",
        "class BasicConvGRUCell(ConvRNNCell):\n",
        "    \"\"\"Basic Conv LSTM recurrent network cell. The\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, shape, filter_size, num_features, forget_bias=1.0, input_size=None,\n",
        "                 state_is_tuple=False, activation=tf.nn.tanh):\n",
        "        \"\"\"Initialize the basic Conv LSTM cell.\n",
        "        Args:\n",
        "          shape: int tuple thats the height and width of the cell\n",
        "          filter_size: int tuple thats the height and width of the filter\n",
        "          num_features: int thats the depth of the cell\n",
        "          forget_bias: float, The bias added to forget gates (see above).\n",
        "          input_size: Deprecated and unused.\n",
        "          state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
        "            the `c_state` and `m_state`.  If False, they are concatenated\n",
        "            along the column axis.  The latter behavior will soon be deprecated.\n",
        "          activation: Activation function of the inner states.\n",
        "        \"\"\"\n",
        "        # if not state_is_tuple:\n",
        "        # logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n",
        "        #             \"deprecated.  Use state_is_tuple=True.\", self)\n",
        "        if input_size is not None:\n",
        "            tf.logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
        "        self.shape = shape\n",
        "        self.filter_size = filter_size\n",
        "        self.num_features = num_features\n",
        "        self._forget_bias = forget_bias\n",
        "        self._state_is_tuple = state_is_tuple\n",
        "        self._activation = activation\n",
        "        self.trainable_var_collection = []\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return (tf.contrib.rnn.LSTMStateTuple(self._num_units, self._num_units)\n",
        "                if self._state_is_tuple else 2 * self._num_units)\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return self._num_units\n",
        "\n",
        "    def __call__(self, inputs, h, mask_in, mask_h, dp_in, dp_h, scope=None):\n",
        "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
        "        # Parameters of gates are concatenated into one multiply for efficiency.\n",
        "        assert mask_in.get_shape().as_list()[:-1] == inputs.get_shape().as_list() and mask_h.get_shape().as_list()[\n",
        "                                                                                      :-1] == inputs.get_shape().as_list()\n",
        "        # print(mask_in.get_shape().as_list()[-1])\n",
        "        assert mask_in.get_shape().as_list()[-1] == 3 and mask_h.get_shape().as_list()[-1] == 3\n",
        "\n",
        "\n",
        "\n",
        "            # concat = self._conv_linear([inputs, h], self.filter_size, self.num_features * 4, True,scope = scope)\n",
        "\n",
        "            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
        "            # i, j, f, o = tf.split(axis=3, num_or_size_splits=4, value=concat)\n",
        "\n",
        "        z_h = self._conv_linear([h * mask_h[..., 0]], self.filter_size, self.num_features, False,\n",
        "                                scope=scope + 'h1') * (1 / dp_h)\n",
        "        r_h = self._conv_linear([h * mask_h[..., 1]], self.filter_size, self.num_features, False,\n",
        "                                scope=scope + 'h2') * (1 / dp_h)\n",
        "        th_h = self._conv_linear([h * mask_h[..., 2]], self.filter_size, self.num_features, False,\n",
        "                                scope=scope + 'h3') * (1 / dp_h)\n",
        "\n",
        "\n",
        "        z_in = self._conv_linear([inputs * mask_in[..., 0]], self.filter_size, self.num_features, True,\n",
        "                                 scope=scope + 'i1') * (1 / dp_in)\n",
        "        r_in = self._conv_linear([inputs * mask_in[..., 1]], self.filter_size, self.num_features, True,\n",
        "                                 scope=scope + 'i2') * (1 / dp_in)\n",
        "        th_in = self._conv_linear([inputs * mask_in[..., 2]], self.filter_size, self.num_features, True,\n",
        "                                 scope=scope + 'i3') * (1 / dp_in)\n",
        "\n",
        "        z_t = tf.nn.sigmoid(z_h + z_in)\n",
        "        r_t = tf.nn.sigmoid(r_h + r_in)\n",
        "        th_t = self._activation(r_t * th_h + th_in)\n",
        "        new_h = (1 - z_t) * h + z_t * th_t\n",
        "        return new_h\n",
        "\n",
        "    def _conv_linear(self, args, filter_size, num_features, bias, bias_start=0.0, scope=None):\n",
        "        \"\"\"convolution:\n",
        "        Args:\n",
        "          args: a 4D Tensor or a list of 4D, batch x n, Tensors.\n",
        "          filter_size: int tuple of filter height and width.\n",
        "          num_features: int, number of features.\n",
        "          bias_start: starting value to initialize the bias; 0 by default.\n",
        "          scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
        "        Returns:\n",
        "          A 4D Tensor with shape [batch h w num_features]\n",
        "        Raises:\n",
        "          ValueError: if some of the arguments has unspecified or wrong shape.\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculate the total size of arguments on dimension 1.\n",
        "        total_arg_size_depth = 0\n",
        "        shapes = [a.get_shape().as_list() for a in args]\n",
        "        for shape in shapes:\n",
        "            if len(shape) != 4:\n",
        "                raise ValueError(\"Linear is expecting 4D arguments: %s\" % str(shapes))\n",
        "            if not shape[3]:\n",
        "                raise ValueError(\"Linear expects shape[4] of arguments: %s\" % str(shapes))\n",
        "            else:\n",
        "                total_arg_size_depth += shape[3]\n",
        "\n",
        "        dtype = [a.dtype for a in args][0]\n",
        "\n",
        "        # Now the computation.\n",
        "        with tf.variable_scope(scope):\n",
        "            matrix = tf.get_variable(\n",
        "                \"weights\", [filter_size[0], filter_size[1], total_arg_size_depth, num_features], dtype=dtype)\n",
        "            if len(args) == 1:\n",
        "                res = tf.nn.conv2d(args[0], matrix, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            else:\n",
        "                res = tf.nn.conv2d(tf.concat(axis=3, values=args), matrix, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            if not bias:\n",
        "                return res\n",
        "            bias_term = tf.get_variable(\n",
        "                \"biases\", [num_features],\n",
        "                dtype=dtype,\n",
        "                initializer=tf.constant_initializer(\n",
        "                    bias_start, dtype=dtype))\n",
        "            if tf.get_variable_scope().reuse == False:\n",
        "                self.trainable_var_collection.append(matrix)\n",
        "                self.trainable_var_collection.append(bias_term)\n",
        "\n",
        "        return res + bias_term\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BasicConvLSTMCell(ConvRNNCell):\n",
        "    \"\"\"Basic Conv LSTM recurrent network cell. The\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, shape, filter_size, num_features, forget_bias=1.0, input_size=None,\n",
        "                 state_is_tuple=False, activation=tf.nn.tanh):\n",
        "        \"\"\"Initialize the basic Conv LSTM cell.\n",
        "        Args:\n",
        "          shape: int tuple thats the height and width of the cell\n",
        "          filter_size: int tuple thats the height and width of the filter\n",
        "          num_features: int thats the depth of the cell\n",
        "          forget_bias: float, The bias added to forget gates (see above).\n",
        "          input_size: Deprecated and unused.\n",
        "          state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
        "            the `c_state` and `m_state`.  If False, they are concatenated\n",
        "            along the column axis.  The latter behavior will soon be deprecated.\n",
        "          activation: Activation function of the inner states.\n",
        "        \"\"\"\n",
        "        # if not state_is_tuple:\n",
        "        # logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n",
        "        #             \"deprecated.  Use state_is_tuple=True.\", self)\n",
        "        if input_size is not None:\n",
        "            tf.logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
        "        self.shape = shape\n",
        "        self.filter_size = filter_size\n",
        "        self.num_features = num_features\n",
        "        self._forget_bias = forget_bias\n",
        "        self._state_is_tuple = state_is_tuple\n",
        "        self._activation = activation\n",
        "        self.trainable_var_collection = []\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return (tf.contrib.rnn.LSTMStateTuple(self._num_units, self._num_units)\n",
        "                if self._state_is_tuple else 2 * self._num_units)\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return self._num_units\n",
        "\n",
        "    def __call__(self, inputs, state, mask_in, mask_h, dp_in, dp_h, scope=None):\n",
        "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
        "        # Parameters of gates are concatenated into one multiply for efficiency.\n",
        "        assert mask_in.get_shape().as_list()[:-1] == inputs.get_shape().as_list() and mask_h.get_shape().as_list()[\n",
        "                                                                                      :-1] == inputs.get_shape().as_list()\n",
        "        # print(mask_in.get_shape().as_list()[-1])\n",
        "        assert mask_in.get_shape().as_list()[-1] == 4 and mask_h.get_shape().as_list()[-1] == 4\n",
        "        if self._state_is_tuple:\n",
        "            c, h = state\n",
        "        else:\n",
        "            c, h = tf.split(axis=3, num_or_size_splits=2, value=state)\n",
        "\n",
        "            # concat = self._conv_linear([inputs, h], self.filter_size, self.num_features * 4, True,scope = scope)\n",
        "\n",
        "            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
        "            # i, j, f, o = tf.split(axis=3, num_or_size_splits=4, value=concat)\n",
        "\n",
        "        i_h = self._conv_linear([h * mask_h[..., 0]], self.filter_size, self.num_features, False,\n",
        "                                scope=scope + 'h1') * (1 / dp_h)\n",
        "        j_h = self._conv_linear([h * mask_h[..., 1]], self.filter_size, self.num_features, False,\n",
        "                                scope=scope + 'h2') * (1 / dp_h)\n",
        "        f_h = self._conv_linear([h * mask_h[..., 2]], self.filter_size, self.num_features, False,\n",
        "                                scope=scope + 'h3') * (1 / dp_h)\n",
        "        o_h = self._conv_linear([h * mask_h[..., 3]], self.filter_size, self.num_features, False,\n",
        "                                scope=scope + 'h4') * (1 / dp_h)\n",
        "        i_in = self._conv_linear([inputs * mask_in[..., 0]], self.filter_size, self.num_features, True,\n",
        "                                 scope=scope + 'i1') * (1 / dp_in)\n",
        "        j_in = self._conv_linear([inputs * mask_in[..., 1]], self.filter_size, self.num_features, True,\n",
        "                                 scope=scope + 'i2') * (1 / dp_in)\n",
        "        f_in = self._conv_linear([inputs * mask_in[..., 2]], self.filter_size, self.num_features, True,\n",
        "                                 scope=scope + 'i3') * (1 / dp_in)\n",
        "        o_in = self._conv_linear([inputs * mask_in[..., 3]], self.filter_size, self.num_features, True,\n",
        "                                 scope=scope + 'i4') * (1 / dp_in)\n",
        "\n",
        "        new_c = (c * tf.nn.sigmoid(f_h + f_in + self._forget_bias) + tf.nn.sigmoid(i_h + i_in) *\n",
        "                 self._activation(j_h + j_in))\n",
        "        new_h = self._activation(new_c) * tf.nn.sigmoid(o_h + o_in)\n",
        "\n",
        "        if self._state_is_tuple:\n",
        "            new_state = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n",
        "        else:\n",
        "            new_state = tf.concat(axis=3, values=[new_c, new_h])\n",
        "        return new_h, new_state\n",
        "\n",
        "    def _conv_linear(self, args, filter_size, num_features, bias, bias_start=0.0, scope=None):\n",
        "        \"\"\"convolution:\n",
        "        Args:\n",
        "          args: a 4D Tensor or a list of 4D, batch x n, Tensors.\n",
        "          filter_size: int tuple of filter height and width.\n",
        "          num_features: int, number of features.\n",
        "          bias_start: starting value to initialize the bias; 0 by default.\n",
        "          scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
        "        Returns:\n",
        "          A 4D Tensor with shape [batch h w num_features]\n",
        "        Raises:\n",
        "          ValueError: if some of the arguments has unspecified or wrong shape.\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculate the total size of arguments on dimension 1.\n",
        "        total_arg_size_depth = 0\n",
        "        shapes = [a.get_shape().as_list() for a in args]\n",
        "        for shape in shapes:\n",
        "            if len(shape) != 4:\n",
        "                raise ValueError(\"Linear is expecting 4D arguments: %s\" % str(shapes))\n",
        "            if not shape[3]:\n",
        "                raise ValueError(\"Linear expects shape[4] of arguments: %s\" % str(shapes))\n",
        "            else:\n",
        "                total_arg_size_depth += shape[3]\n",
        "\n",
        "        dtype = [a.dtype for a in args][0]\n",
        "\n",
        "        # Now the computation.\n",
        "        with tf.variable_scope(scope):\n",
        "            matrix = tf.get_variable(\n",
        "                \"weights\", [filter_size[0], filter_size[1], total_arg_size_depth, num_features], dtype=dtype)\n",
        "            if len(args) == 1:\n",
        "                res = tf.nn.conv2d(args[0], matrix, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            else:\n",
        "                res = tf.nn.conv2d(tf.concat(axis=3, values=args), matrix, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            if not bias:\n",
        "                return res\n",
        "            bias_term = tf.get_variable(\n",
        "                \"biases\", [num_features],\n",
        "                dtype=dtype,\n",
        "                initializer=tf.constant_initializer(\n",
        "                    bias_start, dtype=dtype))\n",
        "            if tf.get_variable_scope().reuse == False:\n",
        "                self.trainable_var_collection.append(matrix)\n",
        "                self.trainable_var_collection.append(bias_term)\n",
        "\n",
        "        return res + bias_term\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls39igd8Gm-E",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJVvAM21Ek2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Net(BasicNet):\n",
        "  # image_size = 448\n",
        "  batch_size = 16\n",
        "  framenum = 16\n",
        "  maximgbatch = 4\n",
        "  init_learning_rate = 10**(-4)\n",
        "  eps = 1e-7\n",
        "  gapnum = 5\n",
        "  salmask_lb = 0.5 #mask cam be salmask_lb~1\n",
        "  dp_in = 0.25\n",
        "  dp_h = 0.25\n",
        "  # num_classes = 20\n",
        "  cell_size = 7\n",
        "  # boxes_per_cell = 2\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__() # init the fatther class of YoloTinyNet\n",
        "    self.global_step = tf.Variable(0, trainable=False)\n",
        "    self.initial_var_collection.append(self.global_step )\n",
        "    self.out = []\n",
        "    self.predict = []\n",
        "    self.loss = []\n",
        "    self.loss_gt = []\n",
        "    self.re = []\n",
        "    self.loss_gt2 = []\n",
        "    self.yolofeatures_colllection = []\n",
        "    self.flowfeatures_colllection = []\n",
        "    self.startflagcnn = True\n",
        "    #process params\n",
        "\n",
        "  def YOLO_tiny_inference(self, images):  # pre128\n",
        "      cnnpretrain = True\n",
        "      cnntrainable = False\n",
        "      self.batch_size = images.get_shape()[0].value\n",
        "      conv_1 = self.conv_layer('conv1', images, 3, 16, stride=1, pretrain=cnnpretrain, batchnormalization=True,\n",
        "                               trainable=cnntrainable)\n",
        "      pool_2 = self.max_pool('pool2', conv_1, 2, stride=2)\n",
        "      conv_3 = self.conv_layer('conv3', pool_2, 3, 32, stride=1, pretrain=cnnpretrain, batchnormalization=True,\n",
        "                               trainable=cnntrainable)\n",
        "      pool_4 = self.max_pool('pool4', conv_3, 2, stride=2)\n",
        "      conv_5 = self.conv_layer('conv5', pool_4, 3, 64, stride=1, pretrain=cnnpretrain, batchnormalization=True,\n",
        "                               trainable=cnntrainable)\n",
        "      pool_6 = self.max_pool('pool6', conv_5, 2, stride=2)\n",
        "      conv_7 = self.conv_layer('conv7', pool_6, 3, 128, stride=1, pretrain=cnnpretrain, batchnormalization=True,\n",
        "                               trainable=cnntrainable)\n",
        "      pool_8 = self.max_pool('pool8', conv_7, 2, stride=2)\n",
        "      conv_9 = self.conv_layer('conv9', pool_8, 3, 256, stride=1, pretrain=cnnpretrain, batchnormalization=True,\n",
        "                               trainable=cnntrainable)\n",
        "      pool_10 = self.max_pool('pool10', conv_9, 2, stride=2)\n",
        "      conv_11 = self.conv_layer('conv11', pool_10, 3, 512, stride=1, pretrain=cnnpretrain, batchnormalization=True,\n",
        "                                trainable=cnntrainable)\n",
        "      pool_12 = self.max_pool('pool12', conv_11, 2, stride=2)\n",
        "      conv_13 = self.conv_layer('conv13', pool_12, 3, 1024, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "      conv_14 = self.conv_layer('conv14', conv_13, 3, 1024, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "      conv_15 = self.conv_layer('conv15', conv_14, 3, 1024, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "      temp_conv = tf.transpose(conv_15, (0, 3, 1, 2))\n",
        "      fc_16 = self.fc_layer('fc16', temp_conv, 256, flat=True, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "      fc_17 = self.fc_layer('fc17', fc_16, 4096, flat=False, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "      fc_18 = self.fc_layer('fc18', fc_17, 1470, flat=False, linear=True, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "\n",
        "      highFeature = tf.reshape(fc_18, [fc_18.get_shape()[0].value, self.cell_size, self.cell_size, -1])\n",
        "\n",
        "      conv_15_2 = self.conv_layer('conv_15_2', conv_15, 1, 128, stride=1,pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "      conv_11_2 = self.conv_layer('conv_11_2', conv_11, 1, 128, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "      conv_9_2 = self.conv_layer('conv_9_2', conv_9, 1, 128, stride=1,pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "      # conv_7_2 = self.conv_layer('conv_7_2', conv_7, 1, 256, stride=1, pretrain=False)\n",
        "      tempsize = conv_9.get_shape().as_list()\n",
        "      newconv_7 = tf.image.resize_images(conv_7, [tempsize[1], tempsize[2]])\n",
        "      newconv_9 = tf.image.resize_images(conv_9_2, [tempsize[1], tempsize[2]])\n",
        "      newconv_11_2 = tf.image.resize_images(conv_11_2, [tempsize[1], tempsize[2]])\n",
        "      newconv_15_2 = tf.image.resize_images(conv_15_2, [tempsize[1], tempsize[2]])\n",
        "      highFeature = tf.image.resize_images(highFeature, [tempsize[1], tempsize[2]])\n",
        "      FeatureMap = tf.concat([newconv_7, newconv_9, newconv_11_2, newconv_15_2, highFeature], axis=3)\n",
        "      weight_mask = tf.constant(self.get_centermask(FeatureMap.get_shape().as_list()), dtype=FeatureMap.dtype)\n",
        "      FeatureMap = FeatureMap * weight_mask\n",
        "      return FeatureMap\n",
        "\n",
        "  def Coarse_salmap(self, Yolofeature):  #  tiny pregen256 fea28_128\n",
        "    cnnpretrain = True\n",
        "    cnntrainable = False\n",
        "    conv_19 = self.conv_layer('conv_19', Yolofeature, 3, 512, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "    conv_20 = self.conv_layer('conv_20', conv_19, 1, 256, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "    conv_21 = self.conv_layer('conv_21', conv_20, 3, 128, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "    conv_22 = self.conv_layer('conv_22', conv_21, 1, 128, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "    deconv_23 = self.transpose_conv_layer('deconv_23', conv_22, 4, 16, stride=2, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "    deconv_24 = self.transpose_conv_layer('deconv_24', deconv_23, 4, 1, stride=2, linear=True, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "\n",
        "    return deconv_24\n",
        "\n",
        "  def Final_inference(self, cat1, cat2):\n",
        "      cnnpretrain = True\n",
        "      cnntrainable = False\n",
        "      MyFeature = tf.concat([cat1, cat2], axis=3)\n",
        "      Lastconv_1 = self.conv_layer('Lastconv_1', MyFeature, 3, 512, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "      Lastconv_2 = self.conv_layer('Lastconv_2', Lastconv_1, 1, 512, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "      Lastconv_3 = self.conv_layer('Lastconv_3', Lastconv_2, 3, 256, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "      Lastconv_4 = self.conv_layer('Lastconv_4', Lastconv_3, 1, 128, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "\n",
        "      return Lastconv_4\n",
        "\n",
        "  def flownet_with_conv(self, x1, x2, mask):\n",
        "        cnnpretrain = True\n",
        "        cnntrainable = False\n",
        "        input = tf.concat([x1, x2], axis=3, name='FNinput')\n",
        "        conv_1 = self.leaky_conv(input, 64, 7, 2, 'FNconv1', pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        conv_1 = self.conv_mask(conv_1, mask)\n",
        "        conv_2 = self.leaky_conv(conv_1, 128, 5, 2, 'FNconv2', pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        conv_2 = self.conv_mask(conv_2, mask)\n",
        "        conv_3 = self.leaky_conv(conv_2, 256, 5, 2, 'FNconv3', pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        conv_3 = self.conv_mask(conv_3, mask)\n",
        "        conv_3_1 = self.leaky_conv(conv_3, 256, 3, 1, 'FNconv3_1', pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        conv_3_1 = self.conv_mask(conv_3_1, mask)\n",
        "        conv_4 = self.leaky_conv(conv_3_1, 512, 3, 2, 'FNconv4', pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        conv_4 = self.conv_mask(conv_4, mask)\n",
        "        conv_4_1 = self.leaky_conv(conv_4, 512, 3, 1, 'FNconv4_1', pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        # conv_4_1 = self.conv_mask(conv_4_1, mask)\n",
        "        conv_5 = self.leaky_conv(conv_4_1, 512, 3, 2, 'FNconv5', pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        # conv_5 = self.conv_mask(conv_5, mask)\n",
        "        conv_5_1 = self.leaky_conv(conv_5, 512, 3, 1, 'FNconv5_1', pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        # conv_5_1 = self.conv_mask(conv_5_1, mask)\n",
        "        conv_6 = self.leaky_conv(conv_5_1, 1024, 3, 2, 'FNconv6', pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        # conv_6 = self.conv_mask(conv_6, mask)\n",
        "        conv_6_1 = self.leaky_conv(conv_6, 1024, 3, 1, 'FNconv6_1', pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        # conv_6_1 = self.conv_mask(conv_6_1, mask)\n",
        "        out_cat_size = conv_4.get_shape().as_list()\n",
        "\n",
        "        Downconv_6_1 = self.conv_layer('FNDownconv_6_1', conv_6_1, 3, 128, stride=1,pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        Downconv_5_1 = self.conv_layer('FNDownconv_5_1', conv_5_1, 3, 128, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        Downconv_4_1 = self.conv_layer('FNDownconv_4_1', conv_4_1, 3, 128, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        Downconv_3_1 = self.conv_layer('FNDownconv_3_1', conv_3_1, 3, 128, stride=1, pretrain=cnnpretrain, trainable=cnntrainable)\n",
        "        conv_6_1_cat = tf.image.resize_images(Downconv_6_1, [out_cat_size[1],out_cat_size[2]])\n",
        "        conv_5_1_cat = tf.image.resize_images(Downconv_5_1, [out_cat_size[1],out_cat_size[2]])\n",
        "        conv_4_1_cat = tf.image.resize_images(Downconv_4_1, [out_cat_size[1],out_cat_size[2]])\n",
        "        conv_3_1_cat = tf.image.resize_images(Downconv_3_1, [out_cat_size[1],out_cat_size[2]])\n",
        "        concat_out = tf.concat([conv_6_1_cat, conv_5_1_cat, conv_4_1_cat, conv_3_1_cat], axis=3, name='FNconcat_out')\n",
        "\n",
        "        return concat_out\n",
        "\n",
        "\n",
        "  def inference(self, videoslides, mask_in, mask_h): #videoslides: [batch framenum h w num_features]\n",
        "    with tf.variable_scope('inference'):\n",
        "        shapes = videoslides.get_shape().as_list()\n",
        "       #shapes2 = GTs.get_shape().as_list()\n",
        "        assert len(shapes)==5\n",
        "        self.batch_size =  videoslides.get_shape()[0].value\n",
        "        #self.framenum = videoslides.get_shape()[1].value\n",
        "        # assert self.framenum % self.maximgbatch == 0 # frmaenum shoube be the multiple of maximgbatch   scope = 'layer_1'\n",
        "        with tf.variable_scope('conv_lstm', initializer=tf.random_uniform_initializer(-.01, 0.1)):\n",
        "            # cell_1 = BasicConvLSTMCell.BasicConvLSTMCell([56, 56], [3, 3], 128, state_is_tuple = False)  # input size,fliter size, input channals\n",
        "            # cell_2 = BasicConvLSTMCell.BasicConvLSTMCell([56, 56], [3, 3], 128, state_is_tuple = False)  # input size,fliter size, input channals\n",
        "            cell_1 = BasicConvLSTMCell([28, 28], [3, 3], 128,\n",
        "                                                         state_is_tuple=False)  # input size,fliter size, input channals\n",
        "            cell_2 = BasicConvLSTMCell([28, 28], [3, 3], 128,\n",
        "                                                         state_is_tuple=False)  # input size,fliter size, input channals\n",
        "\n",
        "            new_state_1 = cell_1.zero_state(self.batch_size, 2, tf.float32)\n",
        "            new_state_2 = cell_2.zero_state(self.batch_size, 2, tf.float32)\n",
        "           # print(videoslides.get_shape().as_list())\n",
        "        for indexframe in range(self.framenum):\n",
        "            frame = videoslides[:, indexframe, ...]\n",
        "            #print(indexframe+self.gapnum)\n",
        "            frame_gap = videoslides[:, indexframe+self.gapnum, ...]\n",
        "            #GTframe = GTs[:, indexframe, ...]\n",
        "            Yolo_features = self.YOLO_tiny_inference(frame)\n",
        "            Presalmap = self.Coarse_salmap(Yolo_features)\n",
        "            if self.startflagcnn == True:\n",
        "                self.yolofeatures_colllection = self.pretrain_var_collection\n",
        "                self.pretrain_var_collection = []\n",
        "            salmask = self._normlized_0to1(Presalmap)\n",
        "            salmask = salmask*(1-self.salmask_lb)+self.salmask_lb\n",
        "            Flow_features = self.flownet_with_conv(frame, frame_gap, salmask)\n",
        "            CNNout = self.Final_inference(Yolo_features, Flow_features)\n",
        "            if self.startflagcnn == True:\n",
        "                self.flowfeatures_colllection = self.pretrain_var_collection\n",
        "            y_1, new_state_1 = cell_1(CNNout, new_state_1,mask_in[...,0:4], mask_h[...,0:4], self.dp_in, self.dp_h, 'lstm_layer1')\n",
        "            y_2, new_state_2 = cell_2(y_1, new_state_2,mask_in[...,4:8], mask_h[...,4:8], self.dp_in, self.dp_h, 'lstm_layer2')\n",
        "            deconv = self.transpose_conv_layer('deconv', y_2, 4, 16, stride=2, pretrain=False, trainable=True)\n",
        "            deconv2 = self.transpose_conv_layer('deconv2', deconv, 4, 1, stride=2, linear=True, pretrain=False, trainable=True)\n",
        "            if self.startflagcnn == True:\n",
        "                tf.get_variable_scope().reuse_variables()\n",
        "                self.trainable_var_collection.extend(cell_1.trainable_var_collection)\n",
        "                self.trainable_var_collection.extend(cell_2.trainable_var_collection)\n",
        "                self.startflagcnn = False\n",
        "\n",
        "            output = self._normlized_0to1(deconv2)\n",
        "            #norm_GT = self._normlized(GTframe)\n",
        "            norm_output = self._normlized(output)\n",
        "           # frame_loss = norm_GT * tf.log(self.eps + norm_GT / (norm_output + self.eps))\n",
        "            #frame_loss = tf.reduce_sum(frame_loss) / norm_GT.get_shape()[0].value\n",
        "           # tf.add_to_collection('losses', frame_loss)\n",
        "            output = tf.expand_dims(output, 1)\n",
        "            if indexframe == 0:\n",
        "                tempout = output\n",
        "            else:\n",
        "                tempout = tf.concat([tempout, output], axis=1)\n",
        "        self.out = tempout\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def _normlized(self, mat): # tensor [batch_size, image_height, image_width, channels] normalize each fea map\n",
        "    mat_shape = mat.get_shape().as_list()\n",
        "    tempsum = tf.reduce_sum(mat, axis=1)\n",
        "    tempsum = tf.reduce_sum(tempsum, axis=1) + self.eps\n",
        "    tempsum = tf.reshape(tempsum, [-1, 1, 1, mat_shape[3]])\n",
        "    return mat / tempsum\n",
        "\n",
        "  def _normlized_0to1(self, mat): # tensor [batch_size, image_height, image_width, channels] normalize each fea map\n",
        "    mat_shape = mat.get_shape().as_list()\n",
        "    tempmin = tf.reduce_min(mat, axis=1)\n",
        "    tempmin= tf.reduce_min(tempmin, axis=1)\n",
        "    tempmin = tf.reshape(tempmin, [-1, 1, 1, mat_shape[3]])\n",
        "    tempmat = mat - tempmin\n",
        "    tempmax = tf.reduce_max(tempmat, axis=1)\n",
        "    tempmax = tf.reduce_max(tempmax, axis=1) + self.eps\n",
        "    tempmax = tf.reshape(tempmax, [-1, 1, 1, mat_shape[3]])\n",
        "    return tempmat / tempmax\n",
        "\n",
        "  def _loss(self):\n",
        "    weight_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES, scope=None)\n",
        "    loss_weight = tf.add_n(weight_loss)\n",
        "    loss_kl = tf.get_collection('losses', scope=None)\n",
        "    loss_kl = tf.add_n(loss_kl)/self.framenum\n",
        "    # self.out = self.predict\n",
        "    tf.summary.scalar('loss_weight', loss_weight)\n",
        "    tf.summary.scalar('loss_kl', loss_kl)\n",
        "    self.loss_gt = loss_kl\n",
        "    self.loss = loss_kl + loss_weight\n",
        "\n",
        "\n",
        "\n",
        "  def _train(self):\n",
        "    # learning_rate = tf.train.exponential_decay(self.init_learning_rate, self.global_step,\n",
        "    #                                            100000, 0.95, staircase=True)\n",
        "    # with tf.variable_scope('trainer'):\n",
        "        opt = tf.train.AdamOptimizer(self.init_learning_rate,beta1=0.9, beta2=0.999, epsilon=1e-08)\n",
        "        # for var in self.trainable_var_collection:\n",
        "        #   print(var.op.name)\n",
        "        grads = opt.compute_gradients(self.loss,var_list = self.trainable_var_collection)\n",
        "        apply_gradient_op = opt.apply_gradients(grads, global_step=self.global_step)\n",
        "        #apply_gradient_op = tf.train.AdamOptimizer(self.init_learning_rate).minimize(self.loss)\n",
        "        self.train = apply_gradient_op\n",
        "        return apply_gradient_op\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbVFeEzAH8ir",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6C78DzVMC-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "498e60de-bbe2-4e34-8b5f-59855c214b50"
      },
      "source": [
        "cd /content/drive/My Drive/model/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/model/pretrain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pon5co8uMFL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9868b62a-ab40-4692-c8ef-591441a558d3"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMconv_prefinal_loss05_dp075_075MC100-200000.index\n",
            "LSTMconv_prefinal_loss05_dp075_075MC100-200000.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH7zbTRJH47a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "57dce602-2bd9-441e-a0ae-7aa62640b70c"
      },
      "source": [
        "batch_size = 1\n",
        "framesnum = 16\n",
        "inputDim = 448\n",
        "input_size = (inputDim, inputDim)\n",
        "resize_shape = input_size\n",
        "outputDim = 112\n",
        "output_size = (outputDim, outputDim)\n",
        "epoch_num = 15\n",
        "overlapframe = 5 #0~framesnum+frame_skip\n",
        "\n",
        "random.seed(a=730)\n",
        "tf.set_random_seed(730)\n",
        "frame_skip = 5\n",
        "dp_in = 1\n",
        "dp_h = 1\n",
        "\n",
        "targetname = 'LSTMconv_prefinal_loss05_dp075_075MC100-200000'\n",
        "VideoName = '/content/drive/My Drive/model/test/cont_5.mp4'\n",
        "CheckpointFile = '/content/drive/My Drive/model/pretrain/'+ targetname\n",
        "\n",
        "def _BatchExtraction(VideoCap, batchsize=batch_size, last_input=None, video_start = True):\n",
        "\tif video_start:\n",
        "\t\t_, frame = VideoCap.read()\n",
        "\t\tframe = cv2.resize(frame, resize_shape)\n",
        "\t\tframe = frame.astype(np.float32)\n",
        "\t\tframe = frame / 255.0 * 2 - 1\n",
        "\t\tframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\t\tInput_Batch = frame[np.newaxis, ...]\n",
        "\t\tfor i in range(batchsize - 1):\n",
        "\t\t\t_, frame = VideoCap.read()\n",
        "\t\t\tframe = cv2.resize(frame, resize_shape)\n",
        "\t\t\tframe = frame.astype(np.float32)\n",
        "\t\t\tframe = frame / 255.0 * 2 - 1\n",
        "\t\t\tframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)     \n",
        "\t\t\tframe = frame[np.newaxis, ...]       \n",
        "\t\t\tInput_Batch = np.concatenate((Input_Batch, frame), axis=0)\n",
        "\t\tInput_Batch = Input_Batch[np.newaxis, ...]       \n",
        "\telse:\n",
        "\t\tInput_Batch = last_input[:,-overlapframe:,...]      \n",
        "\t\tfor i in range(batchsize-overlapframe):\n",
        "\t\t\t_, frame = VideoCap.read()   \n",
        "\t\t\tframe = cv2.resize(frame, resize_shape)\n",
        "\t\t\tframe = frame.astype(np.float32)\n",
        "\t\t\tframe = frame / 255.0 * 2 - 1\n",
        "\t\t\tframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)          \n",
        "\t\t\tframe = frame[np.newaxis,np.newaxis, ...]         \n",
        "\t\t\tInput_Batch = np.concatenate((Input_Batch, frame), axis=1)\n",
        "\treturn Input_Batch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tf.reset_default_graph()\n",
        "net = Net()\n",
        "net.is_training = False\n",
        "input = tf.placeholder(tf.float32, (batch_size, framesnum + frame_skip, input_size[0], input_size[1], 3))\n",
        "RNNmask_in = tf.placeholder(tf.float32, (batch_size, 28, 28, 128, 4 * 2))\n",
        "RNNmask_h = tf.placeholder(tf.float32, (batch_size, 28, 28, 128, 4 * 2))\n",
        "net.inference(input, RNNmask_in, RNNmask_h)\n",
        "net.dp_in = dp_in\n",
        "net.dp_h = dp_h\n",
        "predicts = net.out\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n",
        "# config = tf.ConfigProto()\n",
        "# config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
        "# sess = tf.Session(config=config)\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(sess, CheckpointFile)\n",
        "VideoName_short = VideoName[:-4]\n",
        "VideoCap = cv2.VideoCapture(VideoName)\n",
        "  \n",
        "VideoSize = (int(VideoCap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(VideoCap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "VideoFrame = int(VideoCap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print('New video: %s with %d frames and size of (%d, %d)' % (VideoName_short, VideoFrame, VideoSize[1], VideoSize[0]))\n",
        "fps = float(VideoCap.get(cv2.CAP_PROP_FPS))\n",
        "videoWriter = cv2.VideoWriter(VideoName_short + '_out.avi', cv2.VideoWriter_fourcc('D', 'I', 'V', 'X'),fps,\toutput_size, isColor=False)\n",
        "#cv2.VideoWriter_fourcc('D', 'I', 'V', 'X'),\n",
        "start_time = time.time()\n",
        "videostart = True\n",
        "while VideoCap.get(cv2.CAP_PROP_POS_FRAMES) < VideoFrame - framesnum - frame_skip + overlapframe:\n",
        "  if videostart:\n",
        "    Input_Batch = _BatchExtraction(VideoCap, framesnum + frame_skip, video_start=videostart)\n",
        "    videostart = False\n",
        "    Input_last = Input_Batch\n",
        "  else:\n",
        "    Input_Batch = _BatchExtraction(VideoCap, framesnum + frame_skip, last_input=Input_last,video_start=videostart)\n",
        "    Input_last = Input_Batch\n",
        "\n",
        "  mask_in = np.ones((1, 28, 28, 128, 4 * 2))\n",
        "  mask_h = np.ones((1, 28, 28, 128, 4 * 2))\n",
        "  np_predict = sess.run(predicts, feed_dict={input: Input_Batch, RNNmask_in: mask_in, RNNmask_h: mask_h})\n",
        "  for index in range(framesnum):\n",
        "    Out_frame = np_predict[0,index, :, :, 0]\n",
        "    Out_frame = Out_frame * 255\n",
        "    Out_frame = np.uint8(Out_frame)\n",
        "    videoWriter.write(Out_frame)\n",
        "    #print(\"write down\")\n",
        "# restFrame = VideoFrame - VideoCap.get(cv2.CAP_PROP_POS_FRAMES)\n",
        "#overlap = framesnum - restFrame + overlapframe\n",
        "#Input_Batch = _BatchExtraction(VideoCap, framesnum + frame_skip, last_input=Input_last,video_start=False)\n",
        "#mask_in = np.ones((1, 28, 28, 128, 4 * 2))\n",
        "#mask_h = np.ones((1, 28, 28, 128, 4 * 2))\n",
        "#np_predict = sess.run(predicts, feed_dict={input: Input_Batch, RNNmask_in: mask_in, RNNmask_h: mask_h})\n",
        "#for index in range(restFrame):\n",
        "#\tOut_frame = np_predict[0, framesnum - restFrame + index, :, :, 0]\n",
        "#\tOut_frame = Out_frame * 255\n",
        "#\tOut_frame = np.uint8(Out_frame)\n",
        "#\tvideoWriter.write(Out_frame)\n",
        "    \n",
        "duration = float(time.time() - start_time)\n",
        "print('Total time for this video %f' % (duration))\n",
        "  # print(duration)\n",
        "VideoCap.release()\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/model/pretrain/LSTMconv_prefinal_loss05_dp075_075MC100-200000\n",
            "New video: /content/drive/My Drive/model/test/cont_5 with 1768 frames and size of (1080, 1920)\n",
            "Total time for this video 166.621735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tg92lgh0FrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}